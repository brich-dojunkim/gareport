"""
íŒ¨í„´ ë¶„ì„ ëª¨ë“ˆ
ë³€ìˆ˜ ì¡°í•©ë³„ ì „í™˜ íŒ¨í„´ ë° ìµœì í™” ì¸ì‚¬ì´íŠ¸ ë¶„ì„
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from itertools import combinations
from config import Config

class PatternAnalyzer:
    """íŒ¨í„´ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Config = None):
        """
        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config or Config()
    
    def analyze_conversion_patterns(self, classified_df: pd.DataFrame, 
                                  page_sequences: pd.DataFrame,
                                  conversion_events: pd.DataFrame) -> Dict:
        """
        ì¢…í•©ì ì¸ ì „í™˜ íŒ¨í„´ ë¶„ì„
        
        Args:
            classified_df: í¼ë„ ë‹¨ê³„ê°€ ë¶„ë¥˜ëœ ë°ì´í„°
            page_sequences: í˜ì´ì§€ ì‹œí€€ìŠ¤ ë°ì´í„°
            conversion_events: ì „í™˜ ì´ë²¤íŠ¸ ë°ì´í„°
            
        Returns:
            íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("ğŸ” ì „í™˜ íŒ¨í„´ ì¢…í•© ë¶„ì„ ì‹œì‘...")
        
        patterns = {
            'source_page_combinations': self._analyze_source_page_patterns(classified_df),
            'journey_patterns': self._analyze_user_journey_patterns(page_sequences, conversion_events),
            'engagement_patterns': self._analyze_engagement_patterns(classified_df),
            'temporal_patterns': self._analyze_temporal_patterns(classified_df),
            'high_value_segments': self._identify_high_value_segments(classified_df),
            'conversion_paths': self._analyze_conversion_paths(page_sequences, conversion_events),
            'drop_off_points': self._identify_drop_off_points(classified_df),
            'optimization_insights': {}
        }
        
        # ìµœì í™” ì¸ì‚¬ì´íŠ¸ ìƒì„±
        patterns['optimization_insights'] = self._generate_optimization_insights(patterns)
        
        print("âœ… íŒ¨í„´ ë¶„ì„ ì™„ë£Œ")
        return patterns
    
    def _analyze_source_page_patterns(self, classified_df: pd.DataFrame) -> List[Dict]:
        """ì†ŒìŠ¤-í˜ì´ì§€ ì¡°í•©ë³„ ì „í™˜ íŒ¨í„´ ë¶„ì„"""
        print("  ğŸš€ ì†ŒìŠ¤-í˜ì´ì§€ ì¡°í•© íŒ¨í„´ ë¶„ì„...")
        
        patterns = []
        
        # ì‚¬ìš©ìë³„ ì²« ì†ŒìŠ¤ì™€ ì£¼ìš” í˜ì´ì§€ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        user_patterns = []
        for user_id, user_events in classified_df.groupby('userPseudoId'):
            first_event = user_events.iloc[0]
            converted = 'CONVERSION' in user_events['funnel_stage'].values
            
            # ë°©ë¬¸í•œ í˜ì´ì§€ ì¹´í…Œê³ ë¦¬ë“¤
            page_categories = set()
            for page in user_events['pagePath'].unique():
                category = self.config.get_page_category(page)
                page_categories.add(category)
            
            user_patterns.append({
                'user_id': user_id,
                'source': first_event['sessionSource'],
                'medium': first_event['sessionMedium'],
                'traffic_group': self.config.get_traffic_group(first_event['sessionSource'], first_event['sessionMedium']),
                'page_categories': page_categories,
                'converted': converted,
                'pages_visited': len(user_events['pagePath'].unique()),
                'events_count': len(user_events)
            })
        
        user_patterns_df = pd.DataFrame(user_patterns)
        
        # ì†ŒìŠ¤-í˜ì´ì§€ì¹´í…Œê³ ë¦¬ ì¡°í•© ë¶„ì„
        for traffic_group in user_patterns_df['traffic_group'].unique():
            group_users = user_patterns_df[user_patterns_df['traffic_group'] == traffic_group]
            
            # ì£¼ìš” í˜ì´ì§€ ì¹´í…Œê³ ë¦¬ ì¡°í•© ì°¾ê¸°
            category_combinations = []
            for _, user in group_users.iterrows():
                sorted_categories = sorted(list(user['page_categories']))
                category_combinations.append(tuple(sorted_categories))
            
            # ì¡°í•©ë³„ ì „í™˜ìœ¨ ê³„ì‚°
            combo_stats = {}
            for combo in set(category_combinations):
                combo_users = [user for user, user_combo in zip(group_users.itertuples(), category_combinations) if user_combo == combo]
                if len(combo_users) >= 5:  # ìµœì†Œ 5ëª… ì´ìƒ
                    conversions = sum(1 for user in combo_users if user.converted)
                    conversion_rate = conversions / len(combo_users) * 100
                    
                    combo_stats[combo] = {
                        'combination': ' + '.join(combo),
                        'users': len(combo_users),
                        'conversions': conversions,
                        'conversion_rate': conversion_rate
                    }
            
            if combo_stats:
                best_combo = max(combo_stats.values(), key=lambda x: x['conversion_rate'])
                patterns.append({
                    'traffic_group': traffic_group,
                    'best_combination': best_combo,
                    'total_combinations': len(combo_stats),
                    'group_users': len(group_users)
                })
        
        patterns.sort(key=lambda x: x['best_combination']['conversion_rate'], reverse=True)
        return patterns[:10]  # ìƒìœ„ 10ê°œ
    
    def _analyze_user_journey_patterns(self, page_sequences: pd.DataFrame, 
                                     conversion_events: pd.DataFrame) -> Dict:
        """ì‚¬ìš©ì ì—¬ì • íŒ¨í„´ ë¶„ì„"""
        print("  ğŸ›¤ï¸ ì‚¬ìš©ì ì—¬ì • íŒ¨í„´ ë¶„ì„...")
        
        if page_sequences.empty:
            return {'error': 'í˜ì´ì§€ ì‹œí€€ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        # ì „í™˜í•œ ì‚¬ìš©ìë“¤ì˜ ì—¬ì • ë¶„ì„
        converted_users = set(conversion_events['userPseudoId'].unique()) if not conversion_events.empty else set()
        
        journey_patterns = {
            'common_paths': [],
            'conversion_paths': [],
            'path_lengths': {'converted': [], 'non_converted': []},
            'page_transitions': {}
        }
        
        # ì‚¬ìš©ìë³„ ì—¬ì • ì¶”ì¶œ
        user_journeys = []
        for user_id, user_pages in page_sequences.groupby('userPseudoId'):
            # íƒ€ì„ìŠ¤íƒ¬í”„ ìˆœìœ¼ë¡œ ì •ë ¬
            if 'eventTimestamp' in user_pages.columns:
                user_pages = user_pages.sort_values('eventTimestamp')
            
            journey = list(user_pages['pagePath'].values)
            converted = user_id in converted_users
            
            user_journeys.append({
                'user_id': user_id,
                'journey': journey,
                'length': len(journey),
                'converted': converted,
                'unique_pages': len(set(journey))
            })
        
        user_journeys_df = pd.DataFrame(user_journeys)
        
        # ì—¬ì • ê¸¸ì´ ë¶„ì„
        converted_journeys = user_journeys_df[user_journeys_df['converted']]
        non_converted_journeys = user_journeys_df[~user_journeys_df['converted']]
        
        journey_patterns['path_lengths'] = {
            'converted': {
                'avg_length': converted_journeys['length'].mean() if not converted_journeys.empty else 0,
                'median_length': converted_journeys['length'].median() if not converted_journeys.empty else 0,
                'avg_unique_pages': converted_journeys['unique_pages'].mean() if not converted_journeys.empty else 0
            },
            'non_converted': {
                'avg_length': non_converted_journeys['length'].mean() if not non_converted_journeys.empty else 0,
                'median_length': non_converted_journeys['length'].median() if not non_converted_journeys.empty else 0,
                'avg_unique_pages': non_converted_journeys['unique_pages'].mean() if not non_converted_journeys.empty else 0
            }
        }
        
        # ê³µí†µ ì—¬ì • íŒ¨í„´ ì°¾ê¸° (ì²˜ìŒ 3í˜ì´ì§€)
        first_3_paths = []
        for journey_info in user_journeys:
            if len(journey_info['journey']) >= 3:
                first_3 = tuple(journey_info['journey'][:3])
                first_3_paths.append((first_3, journey_info['converted']))
        
        # íŒ¨í„´ë³„ ì „í™˜ìœ¨ ê³„ì‚°
        path_stats = {}
        for path, converted in first_3_paths:
            if path not in path_stats:
                path_stats[path] = {'total': 0, 'converted': 0}
            path_stats[path]['total'] += 1
            if converted:
                path_stats[path]['converted'] += 1
        
        # ìµœì†Œ 10ëª… ì´ìƒì˜ ì‚¬ìš©ìê°€ ìˆëŠ” íŒ¨í„´ë§Œ ë¶„ì„
        common_paths = []
        for path, stats in path_stats.items():
            if stats['total'] >= 10:
                conversion_rate = stats['converted'] / stats['total'] * 100
                common_paths.append({
                    'path': ' â†’ '.join(path),
                    'users': stats['total'],
                    'conversions': stats['converted'],
                    'conversion_rate': conversion_rate
                })
        
        journey_patterns['common_paths'] = sorted(common_paths, key=lambda x: x['conversion_rate'], reverse=True)[:10]
        
        return journey_patterns
    
    def _analyze_engagement_patterns(self, classified_df: pd.DataFrame) -> Dict:
        """ì°¸ì—¬ë„ íŒ¨í„´ ë¶„ì„"""
        print("  ğŸ“Š ì°¸ì—¬ë„ íŒ¨í„´ ë¶„ì„...")
        
        engagement_patterns = {
            'event_sequences': [],
            'page_category_engagement': {},
            'engagement_conversion_correlation': {}
        }
        
        # ì‚¬ìš©ìë³„ ì´ë²¤íŠ¸ ì‹œí€€ìŠ¤ ë¶„ì„
        user_sequences = []
        for user_id, user_events in classified_df.groupby('userPseudoId'):
            if 'eventTimestamp' in user_events.columns:
                user_events = user_events.sort_values('eventTimestamp')
            
            event_sequence = list(user_events['eventName'].values)
            converted = 'CONVERSION' in user_events['funnel_stage'].values
            
            # ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚°
            engagement_score = self._calculate_engagement_score(user_events)
            
            user_sequences.append({
                'user_id': user_id,
                'event_sequence': event_sequence,
                'converted': converted,
                'engagement_score': engagement_score,
                'total_events': len(user_events)
            })
        
        sequences_df = pd.DataFrame(user_sequences)
        
        # ì°¸ì—¬ë„ì™€ ì „í™˜ ìƒê´€ê´€ê³„
        if not sequences_df.empty:
            converted_engagement = sequences_df[sequences_df['converted']]['engagement_score']
            non_converted_engagement = sequences_df[~sequences_df['converted']]['engagement_score']
            
            engagement_patterns['engagement_conversion_correlation'] = {
                'converted_avg_score': converted_engagement.mean() if not converted_engagement.empty else 0,
                'non_converted_avg_score': non_converted_engagement.mean() if not non_converted_engagement.empty else 0,
                'score_difference': (converted_engagement.mean() - non_converted_engagement.mean()) if not converted_engagement.empty and not non_converted_engagement.empty else 0
            }
        
        # í˜ì´ì§€ ì¹´í…Œê³ ë¦¬ë³„ ì°¸ì—¬ë„
        page_engagement = {}
        for category in self.config.page_categories.keys():
            category_events = classified_df[classified_df['page_category'] == category]
            if not category_events.empty:
                users_in_category = category_events['userPseudoId'].unique()
                converted_users = category_events[category_events['funnel_stage'] == 'CONVERSION']['userPseudoId'].unique()
                
                page_engagement[category] = {
                    'total_users': len(users_in_category),
                    'converted_users': len(converted_users),
                    'conversion_rate': len(converted_users) / len(users_in_category) * 100 if len(users_in_category) > 0 else 0,
                    'avg_events_per_user': len(category_events) / len(users_in_category) if len(users_in_category) > 0 else 0
                }
        
        engagement_patterns['page_category_engagement'] = page_engagement
        
        return engagement_patterns
    
    def _analyze_temporal_patterns(self, classified_df: pd.DataFrame) -> Dict:
        """ì‹œê°„ì  íŒ¨í„´ ë¶„ì„"""
        print("  â° ì‹œê°„ì  íŒ¨í„´ ë¶„ì„...")
        
        if 'eventTimestamp' not in classified_df.columns:
            return {'error': 'íƒ€ì„ìŠ¤íƒ¬í”„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        temporal_patterns = {
            'conversion_timing': {},
            'session_duration_patterns': {},
            'return_visit_patterns': {}
        }
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜
        classified_df = classified_df.copy()
        classified_df['datetime'] = pd.to_datetime(classified_df['eventTimestamp'])
        classified_df['hour'] = classified_df['datetime'].dt.hour
        classified_df['day_of_week'] = classified_df['datetime'].dt.day_name()
        
        # ì „í™˜ê¹Œì§€ ì†Œìš”ì‹œê°„ ë¶„ì„
        conversion_timing = []
        for user_id, user_events in classified_df.groupby('userPseudoId'):
            user_events = user_events.sort_values('datetime')
            first_event = user_events.iloc[0]
            conversion_events = user_events[user_events['funnel_stage'] == 'CONVERSION']
            
            if not conversion_events.empty:
                first_conversion = conversion_events.iloc[0]
                time_to_conversion = (first_conversion['datetime'] - first_event['datetime']).total_seconds() / 3600  # ì‹œê°„ ë‹¨ìœ„
                
                conversion_timing.append({
                    'user_id': user_id,
                    'time_to_conversion_hours': time_to_conversion,
                    'first_source': first_event['sessionSource'],
                    'conversion_hour': first_conversion['hour'],
                    'conversion_day': first_conversion['day_of_week']
                })
        
        if conversion_timing:
            timing_df = pd.DataFrame(conversion_timing)
            temporal_patterns['conversion_timing'] = {
                'avg_time_to_conversion': timing_df['time_to_conversion_hours'].mean(),
                'median_time_to_conversion': timing_df['time_to_conversion_hours'].median(),
                'same_session_conversions': len(timing_df[timing_df['time_to_conversion_hours'] < 1]),
                'within_24h_conversions': len(timing_df[timing_df['time_to_conversion_hours'] < 24])
            }
        
        return temporal_patterns
    
    def _identify_high_value_segments(self, classified_df: pd.DataFrame) -> List[Dict]:
        """ê³ ê°€ì¹˜ ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ ì‹ë³„"""
        print("  ğŸ’ ê³ ê°€ì¹˜ ì„¸ê·¸ë¨¼íŠ¸ ì‹ë³„...")
        
        segments = []
        
        # ì‚¬ìš©ìë³„ íŠ¹ì„± ê³„ì‚°
        user_features = []
        for user_id, user_events in classified_df.groupby('userPseudoId'):
            converted = 'CONVERSION' in user_events['funnel_stage'].values
            
            features = {
                'user_id': user_id,
                'converted': converted,
                'total_events': len(user_events),
                'unique_pages': user_events['pagePath'].nunique(),
                'sessions': user_events['sessionId'].nunique() if 'sessionId' in user_events.columns else 1,
                'source': user_events.iloc[0]['sessionSource'],
                'medium': user_events.iloc[0]['sessionMedium'],
                'traffic_group': self.config.get_traffic_group(user_events.iloc[0]['sessionSource'], user_events.iloc[0]['sessionMedium']),
                'stages_reached': len(set(user_events['funnel_stage'].values) - {'UNKNOWN'}),
                'has_content_engagement': any(stage in user_events['funnel_stage'].values for stage in ['INTEREST']),
                'has_service_exploration': any(stage in user_events['funnel_stage'].values for stage in ['CONSIDERATION'])
            }
            
            user_features.append(features)
        
        users_df = pd.DataFrame(user_features)
        
        # ì„¸ê·¸ë¨¼íŠ¸ë³„ ì „í™˜ìœ¨ ë¶„ì„
        segment_definitions = [
            {
                'name': 'High Engagement + Content Lovers',
                'condition': lambda df: (df['total_events'] >= 10) & (df['has_content_engagement'] == True),
                'description': 'ë†’ì€ ì°¸ì—¬ë„ + ì½˜í…ì¸  ì†Œë¹„'
            },
            {
                'name': 'Multi-Session Explorers',
                'condition': lambda df: df['sessions'] >= 2,
                'description': 'ë‹¤ì¤‘ ì„¸ì…˜ íƒìƒ‰ì'
            },
            {
                'name': 'Paid Traffic High Intent',
                'condition': lambda df: (df['traffic_group'] == 'paid_search') & (df['stages_reached'] >= 3),
                'description': 'ìœ ë£Œ íŠ¸ë˜í”½ ê³ ì˜ë„ ì‚¬ìš©ì'
            },
            {
                'name': 'Content to Service Path',
                'condition': lambda df: (df['has_content_engagement'] == True) & (df['has_service_exploration'] == True),
                'description': 'ì½˜í…ì¸ â†’ì„œë¹„ìŠ¤ íƒìƒ‰ ê²½ë¡œ'
            }
        ]
        
        for segment_def in segment_definitions:
            segment_users = users_df[segment_def['condition'](users_df)]
            
            if len(segment_users) >= 20:  # ìµœì†Œ 20ëª…
                conversion_rate = segment_users['converted'].sum() / len(segment_users) * 100
                
                segments.append({
                    'segment_name': segment_def['name'],
                    'description': segment_def['description'],
                    'users_count': len(segment_users),
                    'conversions': segment_users['converted'].sum(),
                    'conversion_rate': conversion_rate,
                    'avg_events': segment_users['total_events'].mean(),
                    'avg_pages': segment_users['unique_pages'].mean(),
                    'segment_value': conversion_rate * len(segment_users)  # ê°€ì¹˜ ì ìˆ˜
                })
        
        segments.sort(key=lambda x: x['segment_value'], reverse=True)
        return segments[:5]  # ìƒìœ„ 5ê°œ ì„¸ê·¸ë¨¼íŠ¸
    
    def _analyze_conversion_paths(self, page_sequences: pd.DataFrame, 
                                conversion_events: pd.DataFrame) -> Dict:
        """ì „í™˜ ê²½ë¡œ ìƒì„¸ ë¶„ì„"""
        print("  ğŸ¯ ì „í™˜ ê²½ë¡œ ìƒì„¸ ë¶„ì„...")
        
        if page_sequences.empty or conversion_events.empty:
            return {'error': 'í˜ì´ì§€ ì‹œí€€ìŠ¤ ë˜ëŠ” ì „í™˜ ì´ë²¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        conversion_paths = {
            'successful_paths': [],
            'path_analysis': {},
            'critical_pages': []
        }
        
        converted_users = set(conversion_events['userPseudoId'].unique())
        
        # ì „í™˜í•œ ì‚¬ìš©ìë“¤ì˜ ê²½ë¡œ ë¶„ì„
        successful_journeys = []
        for user_id in converted_users:
            user_pages = page_sequences[page_sequences['userPseudoId'] == user_id]
            if not user_pages.empty:
                if 'eventTimestamp' in user_pages.columns:
                    user_pages = user_pages.sort_values('eventTimestamp')
                
                journey = list(user_pages['pagePath'].values)
                successful_journeys.append(journey)
        
        # ê²½ë¡œ íŒ¨í„´ ë¶„ì„
        if successful_journeys:
            # ê°€ì¥ ì¼ë°˜ì ì¸ ì‹œì‘ í˜ì´ì§€
            start_pages = [journey[0] for journey in successful_journeys if journey]
            start_page_counts = pd.Series(start_pages).value_counts()
            
            # ê°€ì¥ ì¼ë°˜ì ì¸ ë§ˆì§€ë§‰ í˜ì´ì§€ (ì „í™˜ ì „)
            second_to_last_pages = [journey[-2] for journey in successful_journeys if len(journey) > 1]
            last_page_counts = pd.Series(second_to_last_pages).value_counts()
            
            conversion_paths['path_analysis'] = {
                'avg_path_length': np.mean([len(journey) for journey in successful_journeys]),
                'most_common_start': start_page_counts.index[0] if not start_page_counts.empty else None,
                'most_common_pre_conversion': last_page_counts.index[0] if not last_page_counts.empty else None,
                'total_conversion_paths': len(successful_journeys)
            }
            
            # ì¤‘ìš”í•œ í˜ì´ì§€ ì‹ë³„ (ì „í™˜ ê²½ë¡œì— ìì£¼ ë“±ì¥í•˜ëŠ” í˜ì´ì§€)
            all_pages_in_paths = []
            for journey in successful_journeys:
                all_pages_in_paths.extend(journey)
            
            page_frequency = pd.Series(all_pages_in_paths).value_counts()
            critical_pages = []
            
            for page, count in page_frequency.head(10).items():
                appearance_rate = count / len(successful_journeys) * 100
                critical_pages.append({
                    'page': page,
                    'appearances': count,
                    'appearance_rate': appearance_rate,
                    'page_category': self.config.get_page_category(page)
                })
            
            conversion_paths['critical_pages'] = critical_pages
        
        return conversion_paths
    
    def _identify_drop_off_points(self, classified_df: pd.DataFrame) -> List[Dict]:
        """ì´íƒˆ ì§€ì  ì‹ë³„"""
        print("  ğŸšª ì´íƒˆ ì§€ì  ì‹ë³„...")
        
        drop_off_points = []
        
        # ì‚¬ìš©ìë³„ ë§ˆì§€ë§‰ ë‹¨ê³„ ë¶„ì„
        user_last_stages = []
        for user_id, user_events in classified_df.groupby('userPseudoId'):
            stages = user_events['funnel_stage'].values
            valid_stages = [s for s in stages if s != 'UNKNOWN']
            
            if valid_stages:
                # ê°€ì¥ ë†’ì€ ë‹¨ê³„ë¥¼ ë§ˆì§€ë§‰ ë‹¨ê³„ë¡œ ì„¤ì •
                stage_order = ['AWARENESS', 'INTEREST', 'CONSIDERATION', 'CONVERSION']
                last_stage = None
                for stage in reversed(stage_order):
                    if stage in valid_stages:
                        last_stage = stage
                        break
                
                if last_stage and last_stage != 'CONVERSION':
                    last_page = user_events.iloc[-1]['pagePath']
                    last_source = user_events.iloc[0]['sessionSource']
                    
                    user_last_stages.append({
                        'user_id': user_id,
                        'last_stage': last_stage,
                        'last_page': last_page,
                        'last_source': last_source,
                        'total_events': len(user_events)
                    })
        
        if user_last_stages:
            last_stages_df = pd.DataFrame(user_last_stages)
            
            # ë‹¨ê³„ë³„ ì´íƒˆ ë¶„ì„
            for stage in ['AWARENESS', 'INTEREST', 'CONSIDERATION']:
                stage_dropoffs = last_stages_df[last_stages_df['last_stage'] == stage]
                
                if not stage_dropoffs.empty:
                    # ê°€ì¥ ë§ì´ ì´íƒˆí•˜ëŠ” í˜ì´ì§€
                    top_dropoff_pages = stage_dropoffs['last_page'].value_counts().head(5)
                    
                    drop_off_points.append({
                        'stage': stage,
                        'total_dropoffs': len(stage_dropoffs),
                        'dropoff_rate': len(stage_dropoffs) / len(last_stages_df) * 100,
                        'top_dropoff_pages': [
                            {'page': page, 'count': count, 'percentage': count/len(stage_dropoffs)*100}
                            for page, count in top_dropoff_pages.items()
                        ]
                    })
        
        return drop_off_points
    
    def _calculate_engagement_score(self, user_events: pd.DataFrame) -> float:
        """ì‚¬ìš©ì ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ì´ë²¤íŠ¸ ë‹¤ì–‘ì„±
        unique_events = user_events['eventName'].nunique()
        score += unique_events * 2
        
        # í˜ì´ì§€ ë‹¤ì–‘ì„±
        unique_pages = user_events['pagePath'].nunique()
        score += unique_pages * 1.5
        
        # ê¹Šì´ ìˆëŠ” ì´ë²¤íŠ¸ (user_engagement, visit_blog ë“±)
        engagement_events = user_events[user_events['eventName'].isin(['user_engagement', 'visit_blog'])]
        score += len(engagement_events) * 3
        
        # ì„¸ì…˜ ìˆ˜ (ì¬ë°©ë¬¸)
        if 'sessionId' in user_events.columns:
            unique_sessions = user_events['sessionId'].nunique()
            score += unique_sessions * 5
        
        return score
    
    def _generate_optimization_insights(self, patterns: Dict) -> Dict:
        """íŒ¨í„´ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì í™” ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        print("  ğŸ’¡ ìµœì í™” ì¸ì‚¬ì´íŠ¸ ìƒì„±...")
        
        insights = {
            'top_opportunities': [],
            'source_optimization': [],
            'content_optimization': [],
            'journey_optimization': [],
            'quick_wins': []
        }
        
        # 1. ì†ŒìŠ¤-í˜ì´ì§€ ì¡°í•© ìµœì í™”
        source_patterns = patterns.get('source_page_combinations', [])
        if source_patterns:
            best_pattern = source_patterns[0]
            insights['source_optimization'].append({
                'type': 'Scale Best Combination',
                'traffic_group': best_pattern['traffic_group'],
                'combination': best_pattern['best_combination']['combination'],
                'current_conversion_rate': best_pattern['best_combination']['conversion_rate'],
                'recommendation': f"{best_pattern['traffic_group']} íŠ¸ë˜í”½ì˜ {best_pattern['best_combination']['combination']} ê²½ë¡œ í™•ëŒ€",
                'priority': 'High'
            })
        
        # 2. ì—¬ì • íŒ¨í„´ ìµœì í™”
        journey_patterns = patterns.get('journey_patterns', {})
        if 'common_paths' in journey_patterns and journey_patterns['common_paths']:
            best_path = journey_patterns['common_paths'][0]
            insights['journey_optimization'].append({
                'type': 'Optimize High-Converting Path',
                'path': best_path['path'],
                'conversion_rate': best_path['conversion_rate'],
                'recommendation': f"'{best_path['path']}' ê²½ë¡œì˜ ì‚¬ìš©ì ê²½í—˜ ê°œì„  ë° ìœ ë„ ê°•í™”",
                'priority': 'Medium'
            })
        
        # 3. ì°¸ì—¬ë„ íŒ¨í„´ ìµœì í™”
        engagement_patterns = patterns.get('engagement_patterns', {})
        if 'page_category_engagement' in engagement_patterns:
            page_engagement = engagement_patterns['page_category_engagement']
            
            # ë†’ì€ ì „í™˜ìœ¨ì„ ë³´ì´ëŠ” í˜ì´ì§€ ì¹´í…Œê³ ë¦¬ ì°¾ê¸°
            high_converting_categories = [
                (category, data) for category, data in page_engagement.items()
                if data['conversion_rate'] > 20 and data['total_users'] > 50
            ]
            
            if high_converting_categories:
                best_category = max(high_converting_categories, key=lambda x: x[1]['conversion_rate'])
                insights['content_optimization'].append({
                    'type': 'Expand High-Converting Content',
                    'category': best_category[0],
                    'conversion_rate': best_category[1]['conversion_rate'],
                    'recommendation': f"{best_category[0]} ì¹´í…Œê³ ë¦¬ ì½˜í…ì¸  í™•ëŒ€ (í˜„ì¬ {best_category[1]['conversion_rate']:.1f}% ì „í™˜ìœ¨)",
                    'priority': 'Medium'
                })
        
        # 4. ê³ ê°€ì¹˜ ì„¸ê·¸ë¨¼íŠ¸ í™œìš©
        high_value_segments = patterns.get('high_value_segments', [])
        if high_value_segments:
            top_segment = high_value_segments[0]
            insights['quick_wins'].append({
                'type': 'Target High-Value Segment',
                'segment': top_segment['segment_name'],
                'conversion_rate': top_segment['conversion_rate'],
                'users_count': top_segment['users_count'],
                'recommendation': f"{top_segment['segment_name']} ì„¸ê·¸ë¨¼íŠ¸ íƒ€ê²ŸíŒ… ê°•í™” ({top_segment['conversion_rate']:.1f}% ì „í™˜ìœ¨)",
                'priority': 'High'
            })
        
        # 5. ì´íƒˆ ì§€ì  ê°œì„ 
        drop_off_points = patterns.get('drop_off_points', [])
        if drop_off_points:
            major_dropoff = max(drop_off_points, key=lambda x: x['total_dropoffs'])
            if major_dropoff['top_dropoff_pages']:
                top_dropoff_page = major_dropoff['top_dropoff_pages'][0]
                insights['quick_wins'].append({
                    'type': 'Fix Major Drop-off Point',
                    'stage': major_dropoff['stage'],
                    'page': top_dropoff_page['page'],
                    'dropoff_count': top_dropoff_page['count'],
                    'recommendation': f"{major_dropoff['stage']} ë‹¨ê³„ì˜ {top_dropoff_page['page']} í˜ì´ì§€ ìµœì í™” í•„ìš”",
                    'priority': 'High'
                })
        
        # ìƒìœ„ ê¸°íšŒ ìš”ì•½
        all_opportunities = (
            insights['source_optimization'] + 
            insights['content_optimization'] + 
            insights['journey_optimization'] + 
            insights['quick_wins']
        )
        
        # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
        priority_order = {'High': 3, 'Medium': 2, 'Low': 1}
        all_opportunities.sort(key=lambda x: priority_order.get(x['priority'], 0), reverse=True)
        
        insights['top_opportunities'] = all_opportunities[:5]
        
        return insights

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì„¤ì • ë° ë¶„ì„ê¸° ì´ˆê¸°í™”
    config = Config()
    analyzer = PatternAnalyzer(config)
    
    # ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    sample_classified = pd.DataFrame({
        'userPseudoId': ['user1', 'user1', 'user2', 'user2'],
        'sessionSource': ['google', 'google', 'naver', 'naver'],
        'sessionMedium': ['cpc', 'cpc', 'organic', 'organic'],
        'pagePath': ['/', '/posts/739', '/skill-guide/one-click', '/provider-join'],
        'funnel_stage': ['AWARENESS', 'INTEREST', 'AWARENESS', 'CONVERSION'],
        'eventName': ['session_start', 'visit_blog', 'session_start', 'íšŒì›ê°€ì…2'],
        'page_category': ['homepage', 'content', 'support', 'authentication'],
        'eventTimestamp': ['2024-01-01 10:00:00', '2024-01-01 10:05:00', 
                          '2024-01-01 14:00:00', '2024-01-01 14:30:00']
    })
    
    sample_sequences = pd.DataFrame({
        'userPseudoId': ['user1', 'user1', 'user2', 'user2'],
        'pagePath': ['/', '/posts/739', '/skill-guide/one-click', '/provider-join'],
        'eventTimestamp': ['2024-01-01 10:00:00', '2024-01-01 10:05:00', 
                          '2024-01-01 14:00:00', '2024-01-01 14:30:00']
    })
    
    sample_conversions = pd.DataFrame({
        'userPseudoId': ['user2'],
        'eventName': ['íšŒì›ê°€ì…2']
    })
    
    # íŒ¨í„´ ë¶„ì„ ì‹¤í–‰
    patterns = analyzer.analyze_conversion_patterns(
        sample_classified, sample_sequences, sample_conversions
    )
    
    print("íŒ¨í„´ ë¶„ì„ ì™„ë£Œ!")
    print(f"ìµœì í™” ê¸°íšŒ: {len(patterns['optimization_insights']['top_opportunities'])}ê°œ")